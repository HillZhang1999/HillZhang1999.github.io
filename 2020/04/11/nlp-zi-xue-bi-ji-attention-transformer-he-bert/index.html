<!DOCTYPE HTML>
<html lang="zh-CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="NLP自学笔记：Attention、Transformer和BERT, 章岳 学习笔记 计算机 算法 程序">
    <meta name="baidu-site-verification" content="fmlEuI34ir">
    <meta name="google-site-verification" content="yCy2azpds5XSuGZvis6OuA-XIGF5GuGpYRAaGfD6o48">
    <meta name="360-site-verification" content="b7c11a830ef90fd1464ad6206bb7b6e7">
    <meta name="description" content="前言由谷歌团队提出的预训练语言模型BERT近年来正在各大自然语言处理任务中屠榜（话说学者们也挺有意思的，模型名都强行凑个芝麻街的人物名，哈哈哈）。 BERT算法的最重要的部分便是Transformer的概念，它本质上是Transformer">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>NLP自学笔记：Attention、Transformer和BERT | HillZhang的博客</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="//stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?46e79e71af0709a5b9106bf20cecc493";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

    
        <script>
            (function(){
                var bp = document.createElement('script');
                var curProtocol = window.location.protocol.split(':')[0];
                if (curProtocol === 'https') {
                    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
                }
                else {
                    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
                }
                var s = document.getElementsByTagName("script")[0];
                s.parentNode.insertBefore(bp, s);
            })();
        </script>
    

    <script>
        (function(){
        var src = "https://jspassport.ssl.qhimg.com/11.0.1.js?d182b3f28525f2db83acfaaf6e696dba";
        document.write('<script src="' + src + '" id="sozz"><\/script>');
        })();
    </script>

<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body>

    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">HillZhang的博客</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>归档</span>
        </a>
    </li>
    
    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="搜索"></i>
        </a>
    </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">HillZhang的博客</div>
        <div class="logo-desc">
            
            苏州大学 | 计算机科学与技术学院 | 软件工程
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                归档
            </a>
        </li>
        
        
    </ul>
</div>

        </div>

        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/28.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        NLP自学笔记：Attention、Transformer和BERT
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 20px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                        <a href="/tags/算法/" target="_blank">
                            <span class="chip bg-color">算法</span>
                        </a>
                        
                        <a href="/tags/机器学习/" target="_blank">
                            <span class="chip bg-color">机器学习</span>
                        </a>
                        
                        <a href="/tags/深度学习/" target="_blank">
                            <span class="chip bg-color">深度学习</span>
                        </a>
                        
                        <a href="/tags/自然语言处理/" target="_blank">
                            <span class="chip bg-color">自然语言处理</span>
                        </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                        <a href="/categories/深度学习/" class="post-category" target="_blank">
                            深度学习
                        </a>
                        
                        <a href="/categories/深度学习/预训练模型/" class="post-category" target="_blank">
                            预训练模型
                        </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2020-04-11
                </div>

                <div class="post-author info-break-policy">
                    <i class="fa fa-user-o fa-fw"></i>作者:&nbsp;&nbsp;
                    
                    HillZhang
                    
                </div>

                
                
                <div class="info-break-policy">
                    <i class="fa fa-file-word-o fa-fw"></i>文章字数:&nbsp;&nbsp;
                    4.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="fa fa-clock-o fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    17 分
                </div>
                
                

                
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>由谷歌团队提出的<strong>预训练语言模型BERT</strong>近年来正在各大自然语言处理任务中屠榜（话说学者们也挺有意思的，模型名都强行凑个芝麻街的人物名，哈哈哈）。 BERT算法的最重要的部分便是<strong>Transformer</strong>的概念，它本质上是Transformer的编码器部分。 而Transformer中使用了<strong>Self-Attention</strong>机制，所以本文会从<strong>Attention机制</strong>说起。</p>
<p>本文是我对Attention、Transformer和BERT的学习总结。同时参考了一些书籍和网上的资料编写而成。写这篇文章的主要目的是加深自己的理解，如有错误欢迎在评论区指出，非常感谢！ </p>
<h2 id="Attention机制"><a href="#Attention机制" class="headerlink" title="Attention机制"></a>Attention机制</h2><p> <strong>注意力模型（Attention Model）</strong> 是深度学习领域最有影响力的思想之一。 它模仿了人类观察文本或图像时的注意力，解决了<strong>长序列问题</strong>，尤其在<strong>机器翻译</strong>等领域应用广泛。</p>
<h3 id="RNN模型如何解决机器翻译问题"><a href="#RNN模型如何解决机器翻译问题" class="headerlink" title="RNN模型如何解决机器翻译问题"></a>RNN模型如何解决机器翻译问题</h3><p>我们以机器翻译问题作为基础，逐步讲解注意力机制及它的优缺点。首先，我们来看RNN模型是如何解决机器翻译问题的。事实上，这在我之前的RNN博客中已经提到过了，这是<strong>一个Many to Many（$T_x!=T_y$）的Seq2Seq序列标注问题。</strong></p>
<p><img src="2.png" alt="机器翻译问题"></p>
<p>下图为RNN解决这一类输入序列和输出序列长度不等的序列标注问题的常用模型结构：</p>
<p><img src="1.png" alt="RNN模型"></p>
<p>可以看到，RNN模型此时被分为了两个部分：<strong>编码器部分（Encoder）和解码器部分（Decoder）。</strong></p>
<p>编码器部分没有Softmax层预测输出，它的作用仅仅是<strong>将源语言文本传入到一个RNN网络（具体可以是GRU、LSTM等），然后在网络的出口处提取最后一个时间步的激活值输出，并且传入解码器。</strong>编码器最后一个时间步的激活值输出因为走过了整个源文本序列，所以可以认为<strong>它蕴含了需要翻译的整个句子的信息</strong>。它的维度与RNN单元的隐藏层神经元数目一致。当然了，这里的RNN可以是深层的，但我们只以单隐藏层的RNN进行讲解。</p>
<p>解码器部分可以看作是一个条件语言模型（Language Model，例如我们常见的AI写诗模型）。它的作用是<strong>通过编码器输入的激活值，生成当前条件下最大概率的目标语言句子</strong>。它与常规的语言模型有两点不同：</p>
<ol>
<li>语言模型零时刻的激活值为<strong>零向量</strong>，而机器翻译模型解码器的零时刻的激活值为<strong>编码器结尾的时间步激活输出。</strong></li>
<li>语言模型为了保证生成句子的多样性，所以每个时间步的输出都是<strong>按照概率分布随机生成</strong>的。而机器翻译模型很明显需要翻译出最准确的结果，所以输出的序列应是<strong>全局最大概率的序列。</strong></li>
</ol>
<p>这个语言模型中，每个RNN单元的输出来自两个方面：</p>
<ul>
<li>前一个时间步RNN单元的激活值输出。</li>
<li>前一个时间步Softmax层预测的输出$\hat y_{t-1}$。</li>
</ul>
<p>顺便提一下，机器翻译问题中的全局最优解问题和CRF、HMM等常规机器学习的序列标注模型中类似，可以使用<strong>维特比算法</strong>来解，在对应的博客中都可以找到相应的说明。如果我们使用贪心算法，将可能陷入局部最优解中。</p>
<p>特别地，我们发现，当词汇表规模很大时，即使是动态规划的维特比算法，其时空复杂度也会很高（时间复杂度：$O(MN^2)$，$Ｍ$为时序数，$N$为词汇表大小）。为了降低计算量，科学家们提出了<strong>集束搜索（Beam Search）</strong>的方法，即第一次输出时选取概率最高的$B$个单词，并将它们作为输入投入第二个时间步，第二次输出时仍然只选概率最高的$B$个单词……以此类推，到最后只会产生$B$条预测序列，我们选取概率最大的的作为最终的结果。<strong>这样做，其实就是在贪心搜索和维特比算法之间进行平衡，当$B=1$时，集束搜索退化成贪心算法，当$B=N$时，集束搜索演变成维特比算法。</strong></p>
<h3 id="RNN-Attention解决机器翻译问题"><a href="#RNN-Attention解决机器翻译问题" class="headerlink" title="RNN+Attention解决机器翻译问题"></a>RNN+Attention解决机器翻译问题</h3><p>上述RNN架构存在着一些问题，例如：<em><em>我们用编码器的最后一个时序的激活值输出作为解码器的初始激活值，也就是说编码器的最后一个激活值向量需要承载源句子的所有信息，这在输入的句子长度变长时，容易成为整个模型的“信息”瓶颈。 </em></em></p>
<p>如下图所示，一个最基本的RNN机器翻译模型，当句子长度变长时，翻译的效果（Bleu得分）将逐渐降低：</p>
<p><img src="3.png" alt="朴素RNN机器翻译模型的缺陷"></p>
<p>为了解决这个问题，科学家们提出了Attention模型，<strong>它将编码器的每个时序隐藏层直接与解码器每个时序的隐藏层相连接，相等于提供了捷径，解码器的预测可以直接利用编码器在每个源文本单词处的编码结果，从而解决了“信息”瓶颈问题。</strong></p>
<p>首先，我们使用一个BRNN来做编码器，每个时序的输出都是该处源文本单词丰富的特征。BRNN可以充分的利用上下文全部的信息。如下图所示，$\hat y_t’$为$t’$时刻RNN单元前向后向激活的向量拼接。</p>
<p><img src="4.png" alt="RNN+Attention的编码器"></p>
<p>接着，我们使用一个RNN来做解码器。RNN的输入除了上一时刻的激活输出向量、上一时刻的预测结果向量，还要加入<strong>注意力输出（Attention Output）</strong>。为了计算解码器在第$t$时刻需要输入的Attention Output，<strong>我们通过给编码器各时序的输出$\hat y_{t’}$一个注意力权重（Attention Weight）$\alpha_{t,t’}$，来对它们进行加权求和。</strong>如下图所示：</p>
<p><img src="5.png" alt="Attention机制"></p>
<p>我们可以将解码器的每个时刻单独拎出来：</p>
<p><img src="6.png" alt="Attention的每个时刻"></p>
<p>那么，如何计算注意力权重呢？一种比较常用的方法是利用解码器RNN上一时刻的激活向量$s_{t-1}$和编码器此时刻的输出向量$a_{t’}$做<strong>点积（dot product）操作</strong>，所得结果记为注意力得分（Attention Score）$e_{t,t’}$。<br>$$<br>e_{t,t’}=s_{t-1}^Ta_{t’}<br>$$<br>显然，注意力权重$\alpha_{t,t’}$需要满足值域在[0,1]，且各时刻$t’$的注意力权重之和为1。所以，我们需要使用Softmax非线性函数进行转化：<br>$$<br>\alpha_{t,t’}=Softmax(e_{t,t’})<br>$$<br>具体流程可以参考cs224n上的这页ppt：</p>
<p><img src="7.png" alt="Attention机制的运算过程"></p>
<p>上面讲解的只是最基本的点积注意力，还有其他几种Attention，本质都是一样的：</p>
<p><img src="8.png" alt="Attention变种"></p>
<p>Attention机制的优点如下：</p>
<ul>
<li>解决了传统RNN架构的<strong>“信息“瓶颈问题</strong>。</li>
<li>让解码器<strong>有选择性地注意</strong>与当前翻译相关的源句子中的词。</li>
<li>通过使编码器的各时序隐藏层和解码器各时序隐藏层<strong>直接相连</strong>，使梯度可以更加直接地进行反向传播，<strong>缓解了梯度消失问题</strong>。</li>
<li>增加了机器翻译模型的<strong>可解释性</strong>。</li>
</ul>
<h2 id="Transformer模型"><a href="#Transformer模型" class="headerlink" title="Transformer模型"></a>Transformer模型</h2><p>尽管RNN+Attention的模型非常有效，但它同时也存在着一些缺陷。RNN最主要的缺陷在于：<strong>它的计算是有时序依赖的，需要用到前一个时间步或者后一个时间步的信息，这导致它难以并行计算，只能串行计算。</strong>而当今时代，GPU的并行化能够大大加速计算过程，如果不能够并行计算，会导致运算速度很低。</p>
<p>为了解决这个问题，科学家们首先提出了使用CNN代替RNN做特征抽取器。通过多层卷积运算，CNN的上层将能够完整地考虑整个时序的信息：</p>
<p><img src="9.png" alt="CNN特征提取"></p>
<p>CNN的优势在于可以并行计算，没有时序依赖。但同时它也有一个缺陷：<strong>CNN需要经过多层计算才能获取长序列的资讯，下层CNN只能看到较小的范围。</strong></p>
<p>为了能够进行并行计算，又不需要多层迭代，科学家们提出了Transformer模型。它的论文题目很霸气《Attention is All You Need》。正如题目所说，<strong>Transformer模型通过采用Self-Attention自注意力机制，完全抛弃了传统RNN在水平方向的传播，只在垂直方向上传播，只需要不断叠加Self-Attention层即可。</strong>这样，每一层的计算都可以并行进行，可以使用GPU进行加速。</p>
<p>你可以使用Self-Attention层来完成任何RNN层可以做到的事情：</p>
<p><img src="10.png" alt="Self-Attention层"></p>
<h3 id="Self-Attention机制"><a href="#Self-Attention机制" class="headerlink" title="Self-Attention机制"></a>Self-Attention机制</h3><p>Self-Attention层的基本结构如下图所示：</p>
<p><img src="11.png" alt="Self-Attention层基本结构"></p>
<p>我们以Self-Attention层的第一个输出$b_1$的计算为例。Self-Attention层计算的流程如下：</p>
<ol>
<li>首先，我们以第一个单词的Word2Vec等词向量作为输入$x_1$。通过与一个参数矩阵$W_a$相乘，我们对$x_1$又进行了一次嵌入，得到了$a_1$向量（这里相当于是一个全连接层）。</li>
<li>接着，我们需要给定三个参数矩阵，分别为$W_q$、$W_k$、和$W_v$，通过三次矩阵乘法，我们从$a_1$计算得到$q_1$、$k_1$、$v_1$三个向量。（相当于三个分开的全连接层）。（需要注意，所有时序的计算共享上述四个参数矩阵）其中，$q$向量称为query向量，用于匹配其它时序的$k$向量；$k$向量称为match向量，用于被其他时序的$q$向量匹配；$v$向量即为当前时序的要被抽取的信息。</li>
<li>然后，与上一节所说的Attention权重计算方式类似。我们先计算输出时序 1 对各输入时序 i 注意力得分$\alpha_{1,i}$，它是由时序1处的query向量$q$和各处的key向量$k$做点积操作后得到：</li>
</ol>
<p>$$<br>\alpha_{1,i}=q_1^Tk_i<br>$$</p>
<p>​     为了保持梯度稳定，我们还要除以向量$q$和向量$k$共同的维度$d$的平方根，这一步叫做Score归一化。<br>$$<br>\alpha_{1,i}=\frac{q_1^Tk_i}{\sqrt{d}}<br>$$</p>
<ol start="4">
<li><p>接着，我们需要使用非线性函数Softmax对注意力分数进行概率转化，得到注意力权重$\hat \alpha_{1,i}$。<br>$$<br>\hat \alpha_{1,i}=\frac{e^{\alpha_{1,i}}}{\sum_{j=1}^{n}e^{\alpha_{1,j}}}<br>$$</p>
</li>
<li><p>最后，我们使用计算得到的注意力权重对各时序的信息向量$v_i$进行加权求和运算，即可得到输出$b_1$。</p>
</li>
</ol>
<p>$$<br>b_1=\sum_{i=1}^n\hat \alpha_{1,i}v_i<br>$$</p>
<p>以上就是Self-Attention层计算的全过程。可以看到，Self-Attention层在水平方向上不会进行任何计算，也就可以使用矩阵进行并行化：<br>$$<br>B=\hat A V=Softmax(\frac{Q^TK}{\sqrt{d}})V<br>$$<br>Self-Attention的优点：</p>
<ul>
<li>因为每个词都和周围所有词做attention，所以任意两个位置都相当于有直连线路，<strong>可捕获长距离依赖。</strong></li>
<li>而且Attention的<strong>可解释性更好</strong>，根据Attention score可以知道一个词和哪些词的关系比较大。</li>
<li><strong>易于并行化</strong>，当前层的Attention计算只和前一层的值有关，所以一层的所有节点可并行执行self-attention操作。</li>
<li>计算效率高，一次Self-Attention只需要<strong>两次矩阵运算，速度很快</strong>。 </li>
</ul>
<p>特别地，如果我们每次产生多组$q$、$k$、$v$向量，这样的Self-Attention层叫做Multi-Head Self-Attention，即多头自注意力机制：</p>
<p><img src="12.png" alt="Multi-Head Self-Attention"></p>
<p>Multi-Head Self-Attention的优点：<strong>不同的head可以关注不同的重点，通过多个head可以关注到更多的信息。</strong>这有些相当于CNN中的不同filter。</p>
<p>此时，Self-Attention层还存在着一个问题：虽然此时通过注意力机制，可以有针对性地捕捉整个句子的信息，但是<strong>没有位置信息</strong>。 也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，<strong>Transformer只是一个功能更强大的词袋模型而已</strong>。 </p>
<p>为了解决这个问题，研究人员中在编码词向量时引入了<strong>位置编码（Position Embedding）的特征</strong>。具体地说，位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。 </p>
<p><img src="13.png" alt="Position Embedding"></p>
<p>上述$e_i$即为第 i 时刻输入的位置信息。它可以是学习而来的，也可以手工设置。</p>
<h3 id="Transformer的架构"><a href="#Transformer的架构" class="headerlink" title="Transformer的架构"></a>Transformer的架构</h3><p>介绍完了Self-Attention机制，我们就可以来看Transformer模型的基本架构了。其结构如下图所示：</p>
<p><img src="14.png" alt="Transformer模型结构"></p>
<p>与传统RNN类似，它由编码器和解码器两部分组成。上面的结构有几点需要解释：</p>
<ul>
<li>Transformer中使用了ResNet中的 residual connection ，即<strong>残差连接</strong>，方便梯度的反向传播。</li>
<li>Add&amp;Norm指的是将残差路径上传来的向量和Self-Attention计算得到的向量相加后进行<strong>Layer-Normalization</strong>，即层标准化。Layer Norm对同一个样本同一层的所有神经元进行标准化，使它们满足标准正态分布，而Batch Norm则是对Batch内不同样本的同一个神经元所有值进行标准化。</li>
<li>编码器输出的两个箭头分别是<strong>输入给解码器第二个Multi-Head Self-Attention层的$k$和$v$，且$k=v$</strong>。相当于提供给解码器源文本信息。</li>
<li>解码器的Masked Multi-Head Self-Attention层之所以叫Masked，是因为由于语言模型的性质，<strong>生成当前词总是需要利用已生成的序列结果，相当于把后面的结果Masked掉了。</strong>所以Transformer解码时，仍然需要按照时序逐个递推计算，只不过大多数内部的计算可以并行化了。</li>
<li>Feed-Forward层就是一个基本的<strong>全连接前馈层</strong>，一般使用<strong>ReLu激活函数</strong>。</li>
</ul>
<p>通过上述架构，我们便可以构建Transformer模型，并且用于各种序列问题。Transformer模型中的注意力机制为翻译模型带来了可解释性，可视化如下：</p>
<p><img src="15.png" alt="Attention可视化"></p>
<h3 id="Transformer的优劣"><a href="#Transformer的优劣" class="headerlink" title="Transformer的优劣"></a>Transformer的优劣</h3><p><strong>优点</strong>：</p>
<ol>
<li>虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。</li>
<li>Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，这对解决NLP中棘手的长期依赖问题是非常有效的。</li>
<li>Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。</li>
<li>算法的并行性非常好，符合目前的硬件（主要指GPU）环境。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li>粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。</li>
<li>Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。</li>
</ol>
<h2 id="BERT模型"><a href="#BERT模型" class="headerlink" title="BERT模型"></a>BERT模型</h2><h3 id="BERT模型介绍"><a href="#BERT模型介绍" class="headerlink" title="BERT模型介绍"></a>BERT模型介绍</h3><p>BERT模型的全名叫做<strong>Bidirectional Encoder Representation from Transformers</strong>。从名称上就可以看出，BERT模型和Transformer模型联系紧密，事实上，它就是Transformer模型的编码器部分。</p>
<p><img src="16.png" alt="BERT模型的结构"></p>
<p>BERT模型本质上是一个<strong>自编码语言模型（Autoencoder LM）</strong>，并且其设计了两个任务来预训练该模型：</p>
<ul>
<li><strong>Masked Language Model</strong>：这个方式的灵感很大程度上来自于Word2Vec模型中的<strong>连续词袋模型CBOW</strong>。与完型填空类似，该方法<strong>在句子中随机遮盖住15%的单词（ 80%的概率替换成[MASK] 、 10%的概率替换成随机的一个词 、10%的概率替换成它本身 ），并且用剩下的单词去预测它们</strong>。如下图所示，我们将需要预测的单词的BERT词向量输出传入一个线性分类器（如Softmax BPNN），即可用交叉熵损失函数训练模型。</li>
</ul>
<p><img src="17.png" alt="Masked LM"></p>
<ul>
<li><strong>Next Sentence Prediction</strong>：该方法的描述是：<strong>给定一篇文章中的两句话，判断第二句话在文本中是否紧跟在第一句话之后，如下图所示</strong>。我们插入[SEP]标志表示句子的分隔符，插入[CLS]表示开始预测的位置。通过预测两个句子正确的连接，即可训练模型。</li>
</ul>
<p><img src="18.png" alt="Next Sentence Prediction"></p>
<p>BERT模型的主要输入是<strong>文本中各个字/词(或者称为token)的原始词向量，该向量既可以随机初始化，也可以利用Word2Vec等算法进行预训练以作为初始值</strong>；输出是<strong>文本中各个字/词融合了全文语义信息后的向量表示 。</strong></p>
<p>此外，BERT模型的输入还有以下两个向量：</p>
<ol>
<li>Transformer模型要求的<strong>位置编码向量（ Position Embeddings ）</strong>：刻画词在句子中的位置信息。</li>
<li><strong>文本向量（Segment Embeddings）</strong>： 该向量的取值在模型训练过程中自动学习，用于刻画文本的全局语义信息，并与单字/词的语义信息相融合 </li>
</ol>
<h3 id="BERT模型的使用"><a href="#BERT模型的使用" class="headerlink" title="BERT模型的使用"></a>BERT模型的使用</h3><p>Word2Vec、GloVe、ELMo等模型是通过语言模型任务得到句子中单词的embedding表示，以此作为补充的新特征给下游任务使用。因为给下游提供的是<strong>每个单词的特征形式</strong>，所以这一类预训练的方法被称为<strong>“Feature-based Pre-Training”</strong>。而BERT模型是“基于<strong>Fine-tuning</strong>的模式”，这种做法和图像领域基于Fine-tuning（微调）的方式基本一致，下游任务需要将模型<strong>改造成BERT模型，才可利用BERT模型预训练好的参数</strong>。 </p>
<p>尽管我们可以从网上下载别人训练好的BERT模型，但是在实际应用在下游任务时，BERT模型的参数依然会不断进行自动微调。不过好处是不需要从头训练了，需要从头开始训练的只有我们下游任务自己的网络参数。</p>
<p>这里，李宏毅老师给出了BERT模型应用在常见的四种下游任务中的模型结构：</p>
<p><img src="19.png" alt="输入是句子，输出是类别"></p>
<p><img src="20.png" alt="输入是句子，输出是句子中每个词的标签"></p>
<p><img src="21.png" alt="输入是两个句子，输出是类别"></p>
<p><img src="22.png" alt="基于抽取的QA"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上就是我对Attention、Transformer和BERT模型的一些学习笔记，目前学的还是比较浅显，日后看到论文或者需要使用的时候再进一步加深理解。</p>

            </div>
            <hr />

            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            

    <div class="reprint" id="reprint-statement">
        <p class="reprint-tip">
            <i class="fa fa-exclamation-triangle"></i>&nbsp;&nbsp;
            <span>转载规则</span>
        </p>
        
            <div class="center-align">
                <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                    <img alt=""
                         style="border-width:0"
                         src="https://i.creativecommons.org/l/by/4.0/88x31.png"/>
                </a>
            </div>
            <br/>
            <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text"
                  property="dct:title" rel="dct:type">
                    《NLP自学笔记：Attention、Transformer和BERT》
                </span> 由
            <a xmlns:cc="http://creativecommons.org/ns#" href="/2020/04/11/nlp-zi-xue-bi-ji-attention-transformer-he-bert/" property="cc:attributionName"
               rel="cc:attributionURL">
                HillZhang
            </a> 采用
            <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                知识共享署名 4.0 国际许可协议
            </a>进行许可。
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>


        </div>
    </div>

    

    

    

    

    
    <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments input[type=text],
    #vcomments input[type=email],
    #vcomments input[type=url],
    #vcomments textarea {
        box-sizing: border-box;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #42b983;
        font-weight: 500;
        text-decoration: underline;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div id="vcomments" class="card-content"></div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<!-- <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script> -->

<script>
    new Valine({
        el: '#vcomments',
        appId: 'uCuovwWUzxnygDdXy6SPs97E-gzGzoHsz',
        appKey: 'oroAoITTPFXepwWrR1qGq7b7',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'false' === 'true',
        avatar: 'wavatar',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: '欢迎和我一起交流！'
    });
</script>

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2020/04/15/shang-xia-wei-guan-xi-huo-qu-lun-wen-yue-du-bi-ji/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/26.jpg" class="responsive-img" alt="上下位关系获取：论文阅读笔记">
                        
                        <span class="card-title">上下位关系获取：论文阅读笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            前言上下位关系在自然语言知识体系中占有非常重要的地位，它是描述事物层次关系的基础，可以作为词语关系网络的核心骨架。根据 WordNet的定义，给定两个词语x和y，如果句子“x是一种|类|个|…|y”可接受，则称y是x的上位词(hyperny
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2020-04-15
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/综述/" class="post-category" target="_blank">
                                    综述
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/机器学习/" target="_blank">
                        <span class="chip bg-color">机器学习</span>
                    </a>
                    
                    <a href="/tags/深度学习/" target="_blank">
                        <span class="chip bg-color">深度学习</span>
                    </a>
                    
                    <a href="/tags/自然语言处理/" target="_blank">
                        <span class="chip bg-color">自然语言处理</span>
                    </a>
                    
                    <a href="/tags/上下位关系/" target="_blank">
                        <span class="chip bg-color">上下位关系</span>
                    </a>
                    
                    <a href="/tags/综述/" target="_blank">
                        <span class="chip bg-color">综述</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/04/08/nlp-zi-xue-bi-ji-word2vec-glove-he-elmo/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/27.jpg" class="responsive-img" alt="NLP自学笔记：Word2Vec、Glove和ELMO">
                        
                        <span class="card-title">NLP自学笔记：Word2Vec、Glove和ELMO</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            前言词汇表示（Word Representation）一直是自然语言处理中最基础也是最重要的任务之一。 深度学习已经给这一领域带来了革命性的变革。其中一个很关键的概念就是词嵌入（word embeddings），这是语言表示的一种方式，可以
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2020-04-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/深度学习/" class="post-category" target="_blank">
                                    深度学习
                                </a>
                            
                            <a href="/categories/深度学习/预训练模型/" class="post-category" target="_blank">
                                    预训练模型
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/算法/" target="_blank">
                        <span class="chip bg-color">算法</span>
                    </a>
                    
                    <a href="/tags/机器学习/" target="_blank">
                        <span class="chip bg-color">机器学习</span>
                    </a>
                    
                    <a href="/tags/深度学习/" target="_blank">
                        <span class="chip bg-color">深度学习</span>
                    </a>
                    
                    <a href="/tags/自然语言处理/" target="_blank">
                        <span class="chip bg-color">自然语言处理</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: HillZhang的博客<br />'
            + '作者: HillZhang<br />'
            + '链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () { bodyElement.removeChild(newdiv); }, 200);
    });
</script>

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fa fa-list"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).slideUp(500);
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).slideDown(500);
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>

<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>
<!-- 代码语言 -->
<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>
<!-- 代码块复制 -->
<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>
<script type="text/javascript" src="/libs/codeBlock/clipboard.min.js"></script>
<!-- 代码块收缩 -->
<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script> 
<!-- 代码块折行 -->
<style type="text/css">code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }</style>


    <footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            &copy; 2019-2020 章岳. 版权所有

            
            &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
            <span class="white-color">291.1k</span>
            

            <br>
            <span id="sitetime"></span>

            
            
        </div>
        <div class="col s12 m4 l4 social-link ">
    <a href="https://github.com/HillZhang1999" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:471791641@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>





    <a href="http://wpa.qq.com/msgrd?v=3&uin=471791641&site=qq&menu=yes" class="tooltipped" target="_blank" data-tooltip="访问我的知乎" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>





    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>
</div>
    </div>
</footer>

<div class="progress-bar"></div>

<!-- 不蒜子计数初始值纠正 -->
<script>
    $(document).ready(function () {

        var int = setInterval(fixCount, 50);
        var pvcountOffset = 80000;
        var uvcountOffset = 20000;

        function fixCount() {
            if (document.getElementById("busuanzi_container_site_pv").style.display != "none") {
                $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + pvcountOffset);
                clearInterval(int);
            }
            if ($("#busuanzi_container_site_pv").css("display") != "none") {
                $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + uvcountOffset); // 加上初始数据 
                clearInterval(int);
            }
        }
    });
</script>

<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
        year - 作为date对象的年份，为4位年份值
        month - 0-11之间的整数，做为date对象的月份
        day - 1-31之间的整数，做为date对象的天数
        hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
        minutes - 0-59之间的整数，做为date对象的分钟数
        seconds - 0-59之间的整数，做为date对象的秒数
        microseconds - 0-999之间的整数，做为date对象的毫秒数 */
        var t1 = Date.UTC(2020, 03, 11, 00, 00, 00); //北京时间2018-2-13 00:00:00
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
        document.getElementById("sitetime").innerHTML = "本站已运行 " + diffYears + " 年 " + diffDays + " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
    }/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
    siteTime();
</script>

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <script type="text/javascript"> var OriginTitile = document.title, st; document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ喔哟，崩溃啦！", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) })
    </script>

    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', '');
</script>



    
    <script src="/libs/others/clicklove.js"></script>
    

    

    <!-- 雪花特效 -->
    

</body>

</html>