<!DOCTYPE HTML>
<html lang="zh-CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="NLP自学笔记：线性模型和对数线性模型, 章岳 学习笔记 计算机 算法 程序">
    <meta name="baidu-site-verification" content="fmlEuI34ir">
    <meta name="google-site-verification" content="yCy2azpds5XSuGZvis6OuA-XIGF5GuGpYRAaGfD6o48">
    <meta name="360-site-verification" content="b7c11a830ef90fd1464ad6206bb7b6e7">
    <meta name="description" content="前言本文是我在学习苏州大学李正华老师的中文信息处理课程（中文信息处理（Chinese Information Processing）Course Resources）时，对于线性模型和对数线性模型的一些个人见解和思考，同时参考了一些书籍和网">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>NLP自学笔记：线性模型和对数线性模型 | HillZhang的博客</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?46e79e71af0709a5b9106bf20cecc493";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

    
        <script>
            (function(){
                var bp = document.createElement('script');
                var curProtocol = window.location.protocol.split(':')[0];
                if (curProtocol === 'https') {
                    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
                }
                else {
                    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
                }
                var s = document.getElementsByTagName("script")[0];
                s.parentNode.insertBefore(bp, s);
            })();
        </script>
    

    <script>
        (function(){
        var src = "https://jspassport.ssl.qhimg.com/11.0.1.js?d182b3f28525f2db83acfaaf6e696dba";
        document.write('<script src="' + src + '" id="sozz"><\/script>');
        })();
    </script>

<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

<body>

    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">HillZhang的博客</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>友情链接</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/contact" class="waves-effect waves-light">
            
            <i class="fa fa-comments"></i>
            
            <span>留言板</span>
        </a>
    </li>
    
    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="搜索"></i>
        </a>
    </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">HillZhang的博客</div>
        <div class="logo-desc">
            
            苏州大学 | 计算机科学与技术学院 | 软件工程
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                友情链接
            </a>
        </li>
        
        <li>
            <a href="/contact" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-comments"></i>
                
                留言板
            </a>
        </li>
        
        
    </ul>
</div>

        </div>

        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/20.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        NLP自学笔记：线性模型和对数线性模型
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 20px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                        <a href="/tags/算法/" target="_blank">
                            <span class="chip bg-color">算法</span>
                        </a>
                        
                        <a href="/tags/机器学习/" target="_blank">
                            <span class="chip bg-color">机器学习</span>
                        </a>
                        
                        <a href="/tags/自然语言处理/" target="_blank">
                            <span class="chip bg-color">自然语言处理</span>
                        </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                        <a href="/categories/机器学习/" class="post-category" target="_blank">
                            机器学习
                        </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2020-03-17
                </div>

                <div class="post-author info-break-policy">
                    <i class="fa fa-user-o fa-fw"></i>作者:&nbsp;&nbsp;
                    
                    HillZhang
                    
                </div>

                
                
                <div class="info-break-policy">
                    <i class="fa fa-file-word-o fa-fw"></i>文章字数:&nbsp;&nbsp;
                    9k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="fa fa-clock-o fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    31 分
                </div>
                
                

                
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文是我在学习苏州大学李正华老师的中文信息处理课程（<a href="http://hlt.suda.edu.cn/~zhli/teach/cip-2015-fall/" target="_blank" rel="noopener">中文信息处理（Chinese Information Processing）Course Resources</a>）时，对于线性模型和对数线性模型的一些个人见解和思考，同时参考了一些书籍和网上的资料编写而成。写这篇文章的主要目的是加深自己对于该模型的理解，如有错误欢迎在评论区指出，非常感谢！</p>
<h2 id="从问题出发"><a href="#从问题出发" class="headerlink" title="从问题出发"></a>从问题出发</h2><p>与隐马尔可夫模型的讲解类似，我们在这里依旧围绕自然语言处理中的经典问题<strong>词性标注问题</strong>进行讨论和解释。</p>
<ul>
<li>给定一个句子X，输出句子中每一个单词对应的词性。</li>
</ul>
<p><img src="1.png" alt="词性标注任务"></p>
<p>与隐马尔可夫模型不同的是，线性模型与对数线性模型将词性标注转化为了一个<strong>多元分类</strong>问题，它们将不再以整个句子序列作为模型的输入，整个句子的词性序列作为模型的输出，而是假设<strong>词语之间的预测相互独立</strong>，将问题转化为<strong>从所有可能的词性类别中选取某个词语最有可能的类别进行输出</strong>。</p>
<h2 id="什么是线性模型"><a href="#什么是线性模型" class="headerlink" title="什么是线性模型"></a>什么是线性模型</h2><p>线性模型（Linear Model)，又称为<strong>线性回归模型</strong>，是一种<strong>有监督学习模型</strong>。我们曾经在高中的时候学习过一种回归方法——线性回归。将这个方法泛化，就可以得到我们的线性模型。</p>
<p>我们以吴恩达老师在coursera上的机器学习课程中的房价预测例子进行说明。</p>
<p><img src="2.png" alt="房价预测"></p>
<p>如上图所示，假设我们得到了一个当地的房价数据集，其中包括了房子的面积、卧室数、层数、房龄等信息，同时给出了对应的房价。我们需要解决的问题就是如何<strong>利用房子的信息预测出它的价格</strong>。在机器学习中，我们把类似于面积、卧室数等这些样本的信息叫做<strong>样本的特征（Feature）</strong>，它们是我们进行预测的依据。我们把需要预测的值叫做<strong>样本的标签（Label)</strong>。训练集中的每一个样本是一个<strong>（特征，标签)二元组</strong>。</p>
<p>在机器学习中，样本的特征往往是人工选择的，例如上述的面积、卧室数、层数等房屋信息。如何选择合适的特征是一门高深的学问，我们称其为<strong>特征工程</strong>。特征往往不止一个，所以使用<strong>向量</strong>的形式对其进行表示。在这里，我使用列向量$\vec x=(x_1,x_2…x_n)$来表示特征向量，由于书写列向量不是很方便，所以只能这么书写，但你需要记住$\vec x$是一个列向量。</p>
<p>当给定某个房子的面积、卧室数、层数、房龄等特征，在线性模型中，我们会使用一个线性方程来计算房子的价格。我们使用$h$表示这个线性方程，$h$代表<strong>hypothesis</strong>(<strong>假设</strong>) ，具体公式如下：<br>$$<br>h_{w}(x)=w_0+w_1x_1+w_2x_2+…+w_nx_n<br>$$<br>我们为每个特征给定一个特征权重$w$，这个权重可以是正数也可以是负数。其中，$w_0$为<strong>偏置项</strong>（bias），专门用来表示方程中的常数项。通过训练样本学习权重$w$，我们便可使用上述方程进行预测。</p>
<p>为了使公式简洁一些，引入$x_0=1$，使用列向量$\vec w=(w_0,w_1…w_n)$来表示权重向量，则公式转化为：<br>$$<br>h_{w}(x)=w_0x_0+w_1x_1+w_2x_2+…+w_nx_n\<br>=w^Tx<br>$$</p>
<h2 id="线性模型的泛化"><a href="#线性模型的泛化" class="headerlink" title="线性模型的泛化"></a>线性模型的泛化</h2><p>看到上述的线性方程，你可能会疑惑：上述方程得到的仅仅是一条直线方程，而需要拟合的数据点往往不能通过简单的直线来拟合。换句话说， 如果仅仅使用在此之前的单元和多元线性回归，<strong>我们只能得到多维空间的高维平面</strong>。为了进一步增强泛化能力，我们可以引入<strong>幂次项特征和多项式特征</strong>。 </p>
<p>例如，我们的模型原先只是一个线性方程：<br>$$<br>h_{w}(x)=w_1x_1+w_2x_2<br>$$<br>为了增强拟合能力，我们可以令$x_3=x_1^2$，$x_4=x_2^2$，$x_5=x_1x_2$。于是，我们的模型将变为：<br>$$<br>h_{w}(x)=w_1x_1+w_2x_2+w_3x_3+w_4x_4+w_5x_5\<br>=w_1x_1+w_2x_2+w_3x_1^2+w_4x_2^2+w_5x_1x_2<br>$$<br>可以看到，此时我们的模型将可以拟合复杂的非线性函数。<strong>也就是说，很多复杂的模型都可以转化为线性模型进行建模。</strong></p>
<p>但需要注意的是：<strong>过多地使用幂次项特征和多项式特征会使模型的拟合能力太强，从而导致过拟合的问题</strong>。我将专门 写一篇blog来讲解机器学习中的过拟合问题，并介绍一些解决过拟合的办法，如正则化和dropout等。</p>
<h2 id="线性模型的特征提取"><a href="#线性模型的特征提取" class="headerlink" title="线性模型的特征提取"></a>线性模型的特征提取</h2><h3 id="连续型特征"><a href="#连续型特征" class="headerlink" title="连续型特征"></a>连续型特征</h3><p>特征分为连续型特征和离散型特征。<strong>连续型特征顾名思义就是取值是连续的特征，例如上述例子中的房子面积。</strong>在实际的机器学习过程中， 很少直接将连续值作为线性回归模型和逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给模型，这样做的优势有以下几点： </p>
<ol>
<li>离散特征的增加和减少都很容易，易于模型的<strong>快速迭代</strong>。</li>
<li>稀疏向量内积乘法运算<strong>速度快</strong>，计算结果<strong>方便存储，容易扩展</strong>。</li>
<li>离散化后的特征对异常数据有很强的<strong>鲁棒性</strong>：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。</li>
<li>线性回归模型和逻辑回归模型属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，<strong>能够提升模型表达能力，加大拟合</strong>。</li>
<li>离散化后可以<strong>进行特征交叉</strong>，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。</li>
<li>特征离散化后，模型会更<strong>稳定</strong>，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。</li>
<li>特征离散化以后，起到了简化模型的作用，降低了模型<strong>过拟合</strong>的风险。</li>
</ol>
<h3 id="离散性特征"><a href="#离散性特征" class="headerlink" title="离散性特征"></a>离散性特征</h3><p><strong>离散型特征顾名思义就是取值是离散的特征，例如上述例子中的房龄、卧室数和层数。</strong>对于离散型特征，我们使用一种叫做<strong>one-hot</strong>的方法进行编码。</p>
<p>one-hot的基本思想：<strong>将离散型特征的每一种取值都看成一种状态</strong>，若你的这一特征中有N个不相同的取值，那么我们就可以将该特征抽象成N种不同的状态，one-hot编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。举个例子，假设我们以学历为例，我们想要研究的类别为小学、中学、大学、硕士、博士五种类别，我们使用one-hot对其编码就会得到：</p>
<blockquote>
<p>小学-&gt;[1,0,0,0,0]</p>
<p>中学-&gt;[0,1,0,0,0]</p>
<p>大学-&gt;[0,0,1,0,0]</p>
<p>硕士-&gt;[0,0,0,1,0]</p>
<p>博士-&gt;[0,0,0,0,1]</p>
</blockquote>
<p><strong>特别地，如果该离散型特征只有True或False两种取值，那么我们通常用1表示True，0表示False</strong>。</p>
<h2 id="线性模型的训练过程"><a href="#线性模型的训练过程" class="headerlink" title="线性模型的训练过程"></a>线性模型的训练过程</h2><h3 id="定义优化目标"><a href="#定义优化目标" class="headerlink" title="定义优化目标"></a>定义优化目标</h3><p>当我们写出了假设函数$h$后，接下来我们要做的就是通过已有的训练集得到合适的模型权重$\vec w=(w_0,w_1…w_n)$。</p>
<p>为了进行权重的训练，首先，我们要定义三个函数：</p>
<ol>
<li><strong>损失函数（Loss Function ）</strong>：<strong>单个样本</strong>的预测值和真实值（标签）的误差。</li>
<li><strong>代价函数（Cost Function ）</strong>：<strong>整个训练集所有样本</strong>的预测值和真实值（标签）的误差，也就是损失函数的平均。</li>
<li><strong>目标函数（Object Function）</strong>：<strong>最终需要优化的函数</strong>。等于经验风险+结构风险（也就是代价函数 + 正则化项）。</li>
</ol>
<p><strong>这里，我们先不讲解正则化的知识，假设我们的目标函数即为代价函数</strong>。那么线性模型的优化问题即为最小化所有样本的误差的平均值。线性模型的误差可以用许多函数来表示，其中比较常用的一种是<strong>均方误差</strong>(Mean squared error) ，这里我们使用均方误差来描述我们的模型误差（PS：更多损失函数可以参考<a href="https://www.jiqizhixin.com/articles/091202" target="_blank" rel="noopener">机器学习中常用的损失函数你知多少？</a>）</p>
<p>假设训练集中样本的标签为$y$，样本的数量为$m$，预测值为假设函数的输出$h_{w}(x)$，则需要优化的目标函数为：<br>$$<br>J(w)=\frac{1}{2m}\sum_{i=1}^{m}(h_w(x^{(i)})-y^{(i)})^2<br>$$<br>我们的目标便是<strong>选择出可以使得建模误差的平方和能够最小的模型参数</strong>，即<strong>找到合适的模型权重$\vec w$，使$J(w)$取得最小值</strong>。需要注意的是，目标函数中的1/2仅仅是为了方便求导。</p>
<h3 id="从极大似然估计到损失函数"><a href="#从极大似然估计到损失函数" class="headerlink" title="从极大似然估计到损失函数"></a>从极大似然估计到损失函数</h3><p>在隐马尔可夫模型的讲解中，我曾经说过损失函数和极大似然估计有着紧密的联系，例如<strong>我们在逻辑回归模型里使用的交叉熵损失函数其实就是似然函数加上负号</strong>。也就是说，<strong>最优化问题既可以理解为最大化模型的似然估计，也可以理解为最小化模型的损失函数</strong>。</p>
<p>为什么会出现这样的现象呢？其实这并不是巧合，是可以通过数学公式进行推导的。具体推导内容可以参考这篇讲义<a href="https://wenku.baidu.com/view/78d16462c9d376eeaeaad1f34693daef5ff713d2.html" target="_blank" rel="noopener">各损失函数与最大似然</a>。</p>
<h3 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h3><h4 id="正规方程法"><a href="#正规方程法" class="headerlink" title="正规方程法"></a>正规方程法</h4><p>正规方程法可以用于求解线性模型的目标函数取得最小值时，权重$w$的准确值。但在机器学习中，我们很少使用这种方法求解模型的权重，我在这里简单提一下它的弊端：</p>
<ol>
<li>对于一个由$n$个训练样本的特征向量$x$组成的训练集矩阵$X$，计算极值点的时间复杂度为$O(n^3)$，<strong>当训练集规模很大时，正规方程法将变得十分缓慢。</strong></li>
<li>正规方程法需要使用训练集矩阵$X$的逆矩阵$X^{(-1)}$，但是<strong>我们无法保证$X$一定可逆。</strong></li>
<li><strong>只适用于线性模型</strong>，不适合逻辑回归模型等其他模型 。</li>
</ol>
<h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>我们已经说过，当训练集规模很大时，例如有上百万、上千万的数据时，正规方程法将会非常缓慢，那么有没有办法能够降低时间复杂度呢？答案是有的，我们在这里可以使用<strong>梯度下降法</strong>进行求解。</p>
<p>为了将问题简化，我们假设模型现在只存在两个特征，对应的权重为$\theta_0$和$\theta_1$。 我们绘制一个等高线图，三个坐标分别为$\theta_0$和$\theta_1$和$J(\theta_0,\theta_1)$： </p>
<p><img src="3.png" alt="等高线图"></p>
<p>则可以看出在三维空间中存在一个使得$J(\theta_0,\theta_1)$最小的点，我们可以发现均方差损失函数的图像很明显是一个<strong>凸函数</strong>，即不存在局部最优解的问题。 </p>
<p>梯度下降法的相当于<strong>我们下山的过程</strong>，每次我们要走一步下山，寻找最低的地方，那么最可靠的方法便是环顾四周，寻找下山速度最快的方向前进，持续该过程，最后便能无限接近最低点。</p>
<p>对于函数而言，<strong>便是求得该函数对所有参数的偏导（梯度），每次根据梯度更新这些参数，直到参数收敛为止，注意这些参数必须同步进行更新</strong>。 从数学上的角度来看，梯度的方向是函数增长速度最快的方向，那么梯度的反方向就是函数减少最快的方向。那么，如果想计算一个函数的最小值，就可以使用梯度下降法的思想来做。 </p>
<p>假设模型共有$n$个特征，加上偏置项的特征$x_0=1$，共有$n+1$个特征。则对于某个权重参数$w_j$进行梯度下降的公式可以表示为：<br>$$<br>w_j=w_j-\alpha\frac{dJ(w)}{dw_j}=w_j-\alpha\frac{1}{m}\sum^{m}_{i=1}(h_w(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$<br>其中，<strong>$\alpha$是一个由我们自己设置的超参数（hyper parameter）——学习率</strong> （<strong>learning rate</strong>），它决定了我们沿着能让目标函数下降程度最大的方向向下迈出的步子有多大。 </p>
<p>梯度下降的过程形象表示便是如下图所示，一步一步下降达到最低点。</p>
<p> <img src="3.jpg" alt="梯度下降的直观表示"></p>
<p>需要说明以下几点：</p>
<ul>
<li><p>$\alpha$为学习率，学习率决定了学习的速度，<strong>我们需要选择合适的学习率</strong>。</p>
<ul>
<li><p>如果$\alpha$过小，那么学习的时间就会很长，导致算法的低效。</p>
</li>
<li><p>如果$\alpha$过大，那么由于每一步更新过大，可能无法收敛到最低点。由于越偏离最低点函数的导数越大，如果$\alpha$过大，某一次更新直接跨越了最低点，来到了比更新之前<strong>更高</strong>的地方。那么下一步更新步会更大，如此反复震荡，离最低点越来越远。</p>
</li>
<li><p>以上两种情况如下图所示 ：</p>
<p><img src="2.jpg" alt></p>
</li>
</ul>
</li>
<li><p><strong>我们的算法不一定能达到最优解</strong>。 但是由于线性回归模型中的函数都是<strong>凸函数</strong>,所以利用梯度下降法，是可以找到全局最优解的，在这里不详细阐述。 </p>
</li>
<li><p>我们这里得到的是<strong>数值解</strong>而非解析解，存在着一定误差。</p>
</li>
</ul>
<p>顺便说一下，我这里举例的是<strong>批量梯度下降（batch gradient descent,BGD）</strong>，即尝试将<strong>所有样本</strong>的误差优化到最小。这种方法的优点是每次下降都是朝着全局最优的方向前进。但是，我们发现它每次都要计算所有样本的损失，这在大批量数据的情况下会非常缓慢。</p>
<p>还有一种梯度下降的方法叫做<strong>随机梯度下降法（ Stochastic gradient descent）</strong>，它每次选取<strong>一个随机样本</strong>，尝试优化它的损失函数。虽然随机梯度下降法每次下降是朝着局部最优的方向前进，无法保证全局的优化，但整体而言，它的下降方向还是朝着全局最优点前进的。与此同时，它的计算量相较于批量梯度下降大大减少了。</p>
<p>此外，还有一种方法叫做<strong>小批量梯度下降法（mini-batch gradient descent）</strong>，这是现在最为常用的一种梯度下降学习法，它是BGD和SGD的折中。它每次尝试优化一个mini-batch-size数量的样本，即减少了运算量，又降低了梯度下降方向出错的可能性。</p>
<h3 id="过拟合与正则化"><a href="#过拟合与正则化" class="headerlink" title="过拟合与正则化"></a>过拟合与正则化</h3><p>在前面我们提到了可以通过引入<strong>幂次项特征和多项式特征</strong>来增加模型的拟合能力。但是，模型的拟合能力过强有时候并不是一件好事。下面，我将介绍线性回归模型中的<strong>过拟合 (over-fitting) 问题</strong>。</p>
<p>首先，我们给出过拟合的定义：</p>
<blockquote>
<p>在统计学中，过拟合（英语：overfitting，或称过度拟合）现象是指在拟合一个统计模型时，使用过多参数。对比于可获取的数据总量来说，一个荒谬的模型只要足够复杂，是可以完美地适应数据。过拟合一般可以视为违反奥卡姆剃刀原则。当可选择的参数的自由度超过数据所包含信息内容时，这会导致最后（拟合后）模型使用任意的参数，这会减少或破坏模型一般化的能力更甚于适应数据。过拟合的可能性不只取决于参数个数和数据，也跟模型架构与数据的一致性有关。此外对比于数据中预期的噪声或错误数量，跟模型错误的数量也有关。 </p>
</blockquote>
<p>通俗的说，过拟合问题的本质是<strong>我们模型的学习能力太强，导致过于强调拟合训练集的样本，从而丢失了泛化的能力</strong>。当模型出现了过拟合的问题，它虽然能非常好地适应我们的训练集，但在新输入变量进行预测时可能会效果不好 。</p>
<p>我们借用吴恩达老师coursera课程中的PPT来描述过拟合、欠拟合、好的拟合三种情况。</p>
<p><img src="4.png" alt="拟合的情况"></p>
<p>在这里，我尝试用一个易于理解的例子来解释上述三种情况：</p>
<blockquote>
<p>我们可以把学生做练习题的过程看成是模型的训练过程，学生在做了很多练习题后，可以参加考试了，我们可以把考试看作为模型的预测过程。欠拟合很简单，就是平时不认真学习，做练习题和参加考试的分数自然都很低，也就是训练集和测试集的准确率都很低。那么过拟合是什么呢？过拟合就是平时只知道死记硬背，仅仅记住了练习题，却不知道举一反三，虽然平时作业分数很高，但是考试成绩却并不好，也就是训练集准确率很高，但测试集准确率很低。最佳的情况就是既认真做练习，又不局限于练习题，这种就是我们虽说的拟合较好的情况。</p>
</blockquote>
<p>为了解决过拟合的问题，我们应该引入一个<strong>参数项</strong>，<strong>在进行梯度下降的时候尽可能使得参数变小</strong>，这样可以使得很多额外的特征的权重接近于0。 这个方法也叫做<strong>正则化</strong>（regularization）。此外，还有一些防止过拟合的办法，例如神经网络中的dropout等，我会在接下来的博客中继续为大家介绍。</p>
<p>正则化中的参数项一般也叫做正则项，常用的正则项有以下两种：</p>
<ul>
<li>L1正则项：各个权重的<strong>绝对值之和</strong>，可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于<strong>特征选择</strong> 。同时，一定程度上 也可以防止过拟合 。</li>
<li>L2正则项：各个权重的<strong>平方和</strong>，可以防止模型过拟合（overfitting）。</li>
</ul>
<p>线性模型和对数线性模型中，我们通常采用的是L2正则项。加入正则项后，模型的代价函数变为：<br>$$<br>J(w)=\frac{1}{2m}[\sum_{i=1}^{m}(h_w(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}w^2]<br>$$<br>梯度下降的公式变为：<br>$$<br>w_j=w_j-\alpha\frac{dJ(w)}{dw_j}=w_j-\alpha\frac{1}{m}\sum^{m}_{i=1}[(h_w(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda w_j]<br>$$<br>我们可以发现，通过引入正则化项，可以使模型的所有参数在梯度下降时多下降一些，从而尽可能地减小。上式中，$\lambda$是一个用于控制正则化程度的超参数，我们一般称其为<strong>正则化系数</strong>，如果$\lambda$过大，则所有权重将趋近于0， 这样我们所得到的只能是一条平行于x轴的直线 。 所以对于正则化，我们要取一个合理的$\lambda$的值 。超参数的选择是机器学习中最为常见的问题，我们一般使用<strong>开发集</strong>进行调参。</p>
<p>顺便提一下，过拟合的情况下，增加训练集的样本数量将会使模型的性能得到提升。在当今这个海量数据的时代，数据是容易获得的，所以深度学习的常见做法是训练出一个拟合能力很强（层数深、神经元多）的神经网络，加上大规模的训练样本，再加上一些正则化的方法，所得出的模型通常性能最好。</p>
<h2 id="回到词性标注问题上来"><a href="#回到词性标注问题上来" class="headerlink" title="回到词性标注问题上来"></a>回到词性标注问题上来</h2><p>相信通过前面的介绍，你已经对线性模型有了基本的认识，下面我们回到词性标注任务上，简单地介绍一下如何基于多元分类的思想使用线性回归模型进行词性标注。</p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p>在我们之前的介绍中，你可能会发现，线性模型主要是用于解决机器学习中的<strong>回归（regression）问题</strong>。也就是说，它预测的最终结果通常是连续值。虽然与对数线性模型相比，线性模型不常用于<strong>分类（classification）</strong>，但在介绍对数线性模型之前，我们仍可以通过一些方法使其运用在分类标注词性上。</p>
<p>最基本的思想就是，<strong>我们通过选取词语的一些特征，训练出一个线性模型，模型的输出为该词语标注为各词性的分值（score），然后我们选取分值最大的词性作为预测的结果。</strong>通过这一转化，我们将线性回归模型运用在了分类问题上。</p>
<h3 id="特征获取"><a href="#特征获取" class="headerlink" title="特征获取"></a>特征获取</h3><p>与之前预测房价的例子不同，词性标注模型的特征不是事先定义好的，而是通过<strong>特征模板（feature template）</strong>在训练集中抽取而来的。特征模板需要专家进行手工的定义。目前，学术界常用的用于词性标注的特征模板如下表所示。</p>
<p><img src="5.png" alt="特征模板"></p>
<p>可以看到，我们一共使用了14种特征模板，其中包含了许多有用的信息，例如词信息，字信息，词缀信息等等。</p>
<p>在实际训练过程中，我们需要先构建<strong>特征空间（feature space）</strong>。它是训练集中所有特征的集合。<strong>构建的方式是：我们每次选取一个词，根据特征模板抽取相应的14种特征，并将其加入到特征空间内。需要注意的是特征空间中不能出现重复的特征。</strong></p>
<p>在这里，我们还可以对特征抽取进行优化。我们观察特征模板可以发现，<strong>对于每个样本，每一种特征的模板都需要抽取每一种词性的特征，而对于所有不同的词性，其特征模板的后缀都是相同的</strong>。通过计算机中经常使用的<strong>段加偏移</strong>的思想，我们可以大大减少构建特征空间所耗费的时间，将时间复杂度由$O(MN)$降为$O(M+N)$。具体的优化操作不再赘述，大家可以参考<a href="http://hlt.suda.edu.cn/~zhli/teach/cip-2015-fall/9-linear-model/main2.pdf" target="_blank" rel="noopener">李老师的讲义</a>。</p>
<p>我在这里画出了这个线性模型的示意图，你应该就能理解为什么它的特征空间可以用二维矩阵的方式来存储，以及它是如何工作的。</p>
<p><img src="6.png" alt="词性标注的线性模型"></p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>对于模型的训练，我们将采用一种叫做<strong>在线学习（Online training）</strong>的方式。这是一种经常用于<strong>推荐系统</strong>的学习算法，方便对模型进行实时训练。你可以把它看作是一种近似于我们之前所提到的随机梯度下降法的学习算法。它每次选取<strong>一个实例</strong>进行训练。由于样本的标签是经过one-hot处理的离散值，而我们模型的输出是取值范围在负无穷到正无穷 ，所以我们在这里无法使用前述的梯度下降算法进行参数的训练。取而代之的是如下的算法。</p>
<p><img src="7.png" alt="Online Training"></p>
<p>简单的讲解一下，可以看到，我们每次选取一个样本进行训练，如果该样本预测正确，那么不修改模型的参数，否则，将该样本与正确词性关联的特征权重全部加1，与错误词性关联的特征权重全部减1。你可以看作是<strong>给预测正确词性的特征一个激励，而给预测错误词性的特征一个惩罚</strong>。</p>
<p>特别地，我们在这里还可以使用上图中$v$作为模型特征权重，这叫做<strong>averaged perceptron</strong>，我经过实验发现使用$v$确实比$w$更好。有关学习算法的优化问题我将专门撰写一篇博客，介绍常见的优化算法，如momentum算法，RMsprop算法和Adam算法的数学原理等。</p>
<h2 id="什么是对数线性模型"><a href="#什么是对数线性模型" class="headerlink" title="什么是对数线性模型"></a>什么是对数线性模型</h2><p>回顾一下我们之前介绍的线性模型，它可以将词性标注转化为一个多分类问题，通过给当前词标注为每种词性的情况进行打分，并选择分值最高的词性作为当前词的标注结果，从而进行词性预测。但这样做其实存在着一些弊端：</p>
<ol>
<li><strong>一些极端值可能会大大影响分类的效果。</strong></li>
<li>对于分类问题，$y$取值为 0 或者1，但如果你使用的是线性模型，那么假设函数的输出值可能远大于 1，或者远小于0，即使所有训练样本的标签都等于 0 或 1。尽管我们知道标签应该取值0 或者1，但是如果算法得到的值远大于1或者远小于0的话，就会感觉很奇怪。<strong>同时，我们也就无法使用损失函数进行梯度下降优化。</strong></li>
</ol>
<p>为了解决上述问题，我们可以使用<strong>对数线性模型（log-linear model）</strong>，或许它的另一个名称更为常用，也可以叫做<strong>逻辑回归模型（logistic regression model）</strong>，不过似乎人们在使用逻辑回归模型时，默认解决的是二分类问题，这里我们称之为<strong>多分类的逻辑回归模型</strong>似乎更合适。</p>
<p>对数线性模型主要做的就是<strong>将线性模型输出的分值使用非线性函数映射到[0，1]区间上，即将分值转化为概率</strong>，这种形式大大减少了极端值的影响，同时概率应用在分类问题上也更易于理解，我们也可以通过样本的真实标签和预测概率定义目标函数，从而使用梯度下降法进行模型训练。</p>
<h2 id="从线性模型到对数线性模型"><a href="#从线性模型到对数线性模型" class="headerlink" title="从线性模型到对数线性模型"></a>从线性模型到对数线性模型</h2><p>为了将分值转化为概率，我们需要使用一些非线性函数进行映射。<strong>比较常用的两种函数分别为：Sigmoid函数和Softmax函数，前者用于二分类的对数线性模型中，而后者用于多分类的对数线性模型中。</strong>顺便提一下，在深度学习中，Sigmoid和Softmax是最为常用的几种<strong>激活函数（activation function）</strong>之一，他们为神经网络增加了非线性特征，从而使神经网络的表达能力大大增强。与此同时，他们也经常是分类任务的神经网络输出层的首选激活函数，其在输出层的作用和在对数线性模型中的作用是一样的，即将分布在任意区间内的分值转化为[0，1]上的概率。</p>
<h3 id="Sigmoid函数（二分类问题）"><a href="#Sigmoid函数（二分类问题）" class="headerlink" title="Sigmoid函数（二分类问题）"></a>Sigmoid函数（二分类问题）</h3><p>我们使用$z$来表示原先的线性模型的假设函数输出（注意：你可能会在别的地方看到式子的最后加上了偏差项$b$，我在这里没有将偏差项特别拎出来，而是为特征向量额外添加了一个维度$x_0=1$，来和权重$w_0$共同表示偏差）：<br>$$<br>z=w^Tx<br>$$<br>则Sigmoid函数的形式为：<br>$$<br>g(z)=\frac{1}{1+e^{-z}}<br>$$<br>可以看到，它的函数图像如下：</p>
<p><img src="4.jpg" alt="Sigmoid"></p>
<p> 可以很明显的看出，该函数将<strong>实数域映射成了[0,1]的区间</strong> ，无论线性模型的输出是什么，我们都可以将其转化为[0，1]上的值，也就是概率。</p>
<p>所以，加入了Sigmoid函数后，对数线性模型的假设函数变为：<br>$$<br>h_w(x)=g(z)=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-w^Tx}}<br>$$</p>
<h3 id="Softmax函数（多分类问题）"><a href="#Softmax函数（多分类问题）" class="headerlink" title="Softmax函数（多分类问题）"></a>Softmax函数（多分类问题）</h3><p>我们依然使用$z$来表示原先的线性模型的假设函数输出，由于多分类问题每个样本会输出多个概率值，每个概率值都是该样本标注为某个词性的概率，我们使用$z_i$来表示当前样本被标注为第$i$种词性的分值，共有$n$种词性。则Softmax函数的形式为：<br>$$<br>g(z_i)=\frac{e^{z_i}}{\sum_{j=1}^{n}{e^{z_j}}}<br>$$<br>所以，加入了Softmax函数后，对数线性模型的假设函数变为：<br>$$<br>h_w(x_i)=g(z_i)=\frac{e^{z_i}}{\sum_{j=1}^{n}{e^{z_j}}}=\frac{e^{w^T_ix}}{\sum_{j=1}^{n}{e^{w^T_jx}}}<br>$$<br>我们发现，该函数实际上对每一个类别 i 的概率进行了归一化处理，从而保证其范围在[0，1]上，此外，它还保证了对于每一个样本，其标注为每一个类别的条件概率之和为1<strong>（这也是另一种多分类方法One vs All所不能做到的）</strong>。实际上，你还可以推导发现，<strong>Sigmoid函数其实是Softmax函数在类别 n=2 时的特殊形式</strong>。</p>
<h2 id="对数线性模型背后的数学原理"><a href="#对数线性模型背后的数学原理" class="headerlink" title="对数线性模型背后的数学原理"></a>对数线性模型背后的数学原理</h2><p>事实上，上述两种非线性映射函数并不是科学家们一拍脑袋就定义出来的，他们都是有严格的概率统计学证明的。Sigmoid函数是Softmax函数在二分类时的特殊形式，而Softmax函数可以通过信息论中的<strong>最大熵模型</strong>进行推导。我们根据<strong>求解最大熵函数在给定的约束条件下的极值（使用拉格朗日乘子法）</strong>，可以得到最大熵模型的参数形式，也就是Softmax函数的形式。</p>
<p>具体证明过程可以参考<a href="http://hlt.suda.edu.cn/~zhli/teach/cip-2015-fall/10-maxent-loglinear/main.pdf" target="_blank" rel="noopener">李老师的讲义</a>。</p>
<h2 id="定义对数线性模型的优化目标"><a href="#定义对数线性模型的优化目标" class="headerlink" title="定义对数线性模型的优化目标"></a>定义对数线性模型的优化目标</h2><p>如果我们在对数线性模型中依然使用线性回归模型的均方差损失函数进行梯度下降，我们将发现<strong>加入了非线性映射函数的均方差损失函数不再是一个凸优化问题，无法保证获得全局最优解</strong>，所以我们需要另寻新的损失函数。</p>
<p>按照我们惯用的做法，我们依然可以通过<strong>极大化似然估计</strong>的方式来求解对数线性模型的优化目标。</p>
<blockquote>
<p><strong>极大似然估计：利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值（模型已定，参数未知）</strong> 。</p>
</blockquote>
<p>由于我们的词性标注问题是多分类问题，所以我在这里就推导一下Softmax函数的目标函数，事实上，Sigmoid函数只是它的一种特殊情况。</p>
<p>我们假设当前词预测为第 i 种词性的概率为$\hat{y_i}$，根据Softmax函数，我们的模型现在是：<br>$$<br>\hat{y_i}=h_w(x_i)=\frac{e^{w^T_ix}}{\sum_{j=1}^{n}{e^{w^T_jx}}}<br>$$<br>我们已知模型的真实概率分布，也就是样本结果信息，即训练集中每一个样本的特征向量和标签。<strong>极大似然估计的目标就是找到一组合适的模型的参数，使得这些样本结果出现的概率最大。</strong>也就是说，对于每一个样本，我们都希望它被标注为正确标签的概率最大。 对于整个训练集，我们当然是期望所有样本的联合概率都达到最大。我们的目标函数，本身是个联合概率，但是假设每个样本独立，那就可以写成连乘形式：<br>$$<br>J(w)=\frac{1}{N}\prod^{N}_{i=1}\prod^{M}_{j=1}(\frac{e^{w^T_jx_i}}{\sum_{k=1}^{M}{e^{w^T_kx_i}}})^{y_{ij}}<br>$$<br>其中，$N$指样本数量，$i$指第几个样本，$M$指类别数量，$j$指第几个类别，$y_{ij}$指第$i$个样本被标注为第$j$个类别的取值是0还是1。</p>
<p>我们还可以发现，对于每一个样本$i$，其标签都是一个One-hot向量，即只有一个正确标签为1，其余均为0，所以我们可以对上述目标函数进行化简。设$y_{i}$为第$i$个样本的正确标签，则目标函数简化为：<br>$$<br>J(w)=\frac{1}{N}\prod^{N}_{i=1}(\frac{e^{w^T_ix_i}}{\sum_{k=1}^{M}{e^{w^T_kx_i}}})^{y_{i}}<br>$$<br>到了这一步，其实我们就可以使用当前的目标函数进行梯度下降，计算模型参数了。但是，我们发现<strong>连乘的形式会使得我们梯度求解变得很复杂，所以为了简化计算，我们可以对目标函数取对数</strong>：<br>$$<br>logJ(w)=\frac{1}{N}\sum^{N}_{i=1}y_{i}log(\frac{e^{w^T_ix_i}}{\sum_{k=1}^{M}{e^{w^T_kx_i}}})=\frac{1}{N}\sum^{N}_{i=1}y_{i}log\hat{y_i}<br>$$<br>以上就是我们极大化似然估计的目标函数，我们只需要使用梯度上升法对它进行优化即可获得最合适的$w$。此外，<strong>我们也可以加上一个负号，将其转化为最小化损失函数的形式，从而进行梯度下降求解</strong>。<br>$$<br>Loss(w)=-\frac{1}{N}\sum^{N}_{i=1}y_{i}log\hat{y_i}<br>$$<br>上面这个式子就是Softmax对数线性模型的损失函数，它其实就是我们常说的<strong>交叉熵（cross entropy）损失函数</strong>，更多有关交叉熵的知识来自于信息论，我在此就不多加介绍了。</p>
<p>特别地，如果你根据上述损失函数计算梯度，<strong>你会发现对于同一个特征权重，交叉熵损失函数的梯度居然神奇地和均方差损失函数一致！</strong>可以自己动手推导一遍。</p>
<h2 id="用对数线性模型解决词性标注问题"><a href="#用对数线性模型解决词性标注问题" class="headerlink" title="用对数线性模型解决词性标注问题"></a>用对数线性模型解决词性标注问题</h2><p>有关对数线性模型以及梯度下降法的知识在之前我已经全部介绍过了。在这里我们只需要应用即可。</p>
<p><strong>依然是使用我们的特征模板构建特征空间，模型使用Softmax函数进行概率转化，模型的训练方式采用随机梯度下降法，损失函数选择交叉熵损失函数。正则化方式选择L2正则化，随机打乱数据集，并使用学习率下降算法，对学习率进行模拟退火处理，使模型在梯度下降后期趋于稳定。</strong></p>
<p>具体代码可以参考 <a href="https://github.com/SUDA-LA/CIP" target="_blank" rel="noopener">https://github.com/SUDA-LA/CIP</a> 。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上就是我在线性模型和对数线性模型学习过程中总结的基础知识，接下来，我还会介绍条件随机场（CRF）模型，并给出其解决词性标注问题的方法。</p>
<h2 id="参考资料（部分）"><a href="#参考资料（部分）" class="headerlink" title="参考资料（部分）"></a>参考资料（部分）</h2><ul>
<li><a href="http://hlt.suda.edu.cn/~zhli/teach/cip-2015-fall/" target="_blank" rel="noopener">苏州大学李正华老师的课件</a></li>
<li>李航老师《统计学习方法》的第六章</li>
<li>Andrew Ng的机器学习课程</li>
<li>Andrew Ng的深度学习课程</li>
</ul>

            </div>
            <hr />

            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            

    <div class="reprint" id="reprint-statement">
        <p class="reprint-tip">
            <i class="fa fa-exclamation-triangle"></i>&nbsp;&nbsp;
            <span>转载规则</span>
        </p>
        
            <div class="center-align">
                <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                    <img alt=""
                         style="border-width:0"
                         src="https://i.creativecommons.org/l/by/4.0/88x31.png"/>
                </a>
            </div>
            <br/>
            <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text"
                  property="dct:title" rel="dct:type">
                    《NLP自学笔记：线性模型和对数线性模型》
                </span> 由
            <a xmlns:cc="http://creativecommons.org/ns#" href="/2020/03/17/nlp-zi-xue-bi-ji-xian-xing-mo-xing-he-dui-shu-xian-xing-mo-xing/" property="cc:attributionName"
               rel="cc:attributionURL">
                HillZhang
            </a> 采用
            <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                知识共享署名 4.0 国际许可协议
            </a>进行许可。
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>


        </div>
    </div>

    

    

    

    

    
    <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments input[type=text],
    #vcomments input[type=email],
    #vcomments input[type=url],
    #vcomments textarea {
        box-sizing: border-box;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #42b983;
        font-weight: 500;
        text-decoration: underline;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div id="vcomments" class="card-content"></div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<!-- <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script> -->

<script>
    new Valine({
        el: '#vcomments',
        appId: 'uCuovwWUzxnygDdXy6SPs97E-gzGzoHsz',
        appKey: 'oroAoITTPFXepwWrR1qGq7b7',
        notify: 'false' === 'true',
        verify: 'false' === 'true',
        visitor: 'false' === 'true',
        avatar: 'wavatar',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: '欢迎和我一起交流！'
    });
</script>

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2020/03/21/jian-zhi-offer-ti-jie/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/25.jpg" class="responsive-img" alt="剑指Offer题解">
                        
                        <span class="card-title">剑指Offer题解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            前言最近随便投了字节跳动的暑期实习生，没想到简历过关了，4.12日先去笔试。考虑到Leetcode上面的题目比较多，决定先把《剑指Offer》上的所有题目都刷一遍。
1. 数组中重复的数字思路：使用哈希表或者集合等数据结构，遍历数组中的数字
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2020-03-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/编程算法/" class="post-category" target="_blank">
                                    编程算法
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/算法/" target="_blank">
                        <span class="chip bg-color">算法</span>
                    </a>
                    
                    <a href="/tags/剑指Offer/" target="_blank">
                        <span class="chip bg-color">剑指Offer</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/03/17/nlp-zi-xue-bi-ji-yin-ma-er-ke-fu-mo-xing/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/0.jpg" class="responsive-img" alt="NLP自学笔记：隐马尔可夫模型">
                        
                        <span class="card-title">NLP自学笔记：隐马尔可夫模型</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            前言本文是我在学习苏州大学李正华老师的中文信息处理课程（中文信息处理（Chinese Information Processing）Course Resources）时，对于隐马尔可夫模型的一些个人见解和思考，同时参考了一些书籍和网上的资料
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2020-03-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/机器学习/" class="post-category" target="_blank">
                                    机器学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/算法/" target="_blank">
                        <span class="chip bg-color">算法</span>
                    </a>
                    
                    <a href="/tags/机器学习/" target="_blank">
                        <span class="chip bg-color">机器学习</span>
                    </a>
                    
                    <a href="/tags/自然语言处理/" target="_blank">
                        <span class="chip bg-color">自然语言处理</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: HillZhang的博客<br />'
            + '作者: HillZhang<br />'
            + '链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () { bodyElement.removeChild(newdiv); }, 200);
    });
</script>

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fa fa-list"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).slideUp(500);
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).slideDown(500);
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>

<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>
<!-- 代码语言 -->
<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>
<!-- 代码块复制 -->
<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>
<script type="text/javascript" src="/libs/codeBlock/clipboard.min.js"></script>
<!-- 代码块收缩 -->
<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script> 
<!-- 代码块折行 -->
<style type="text/css">code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }</style>


    <footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            &copy; 2019-2020 章岳. 版权所有

            
            &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
            <span class="white-color">247.2k</span>
            

            <br>
            <span id="sitetime"></span>

            
            
        </div>
        <div class="col s12 m4 l4 social-link ">
    <a href="https://github.com/HillZhang1999" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:471791641@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>





    <a href="http://wpa.qq.com/msgrd?v=3&uin=471791641&site=qq&menu=yes" class="tooltipped" target="_blank" data-tooltip="访问我的知乎" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>





    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>
</div>
    </div>
</footer>

<div class="progress-bar"></div>

<!-- 不蒜子计数初始值纠正 -->
<script>
    $(document).ready(function () {

        var int = setInterval(fixCount, 50);
        var pvcountOffset = 80000;
        var uvcountOffset = 20000;

        function fixCount() {
            if (document.getElementById("busuanzi_container_site_pv").style.display != "none") {
                $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + pvcountOffset);
                clearInterval(int);
            }
            if ($("#busuanzi_container_site_pv").css("display") != "none") {
                $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + uvcountOffset); // 加上初始数据 
                clearInterval(int);
            }
        }
    });
</script>

<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
        year - 作为date对象的年份，为4位年份值
        month - 0-11之间的整数，做为date对象的月份
        day - 1-31之间的整数，做为date对象的天数
        hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
        minutes - 0-59之间的整数，做为date对象的分钟数
        seconds - 0-59之间的整数，做为date对象的秒数
        microseconds - 0-999之间的整数，做为date对象的毫秒数 */
        var t1 = Date.UTC(2020, 03, 11, 00, 00, 00); //北京时间2018-2-13 00:00:00
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
        document.getElementById("sitetime").innerHTML = "本站已运行 " + diffYears + " 年 " + diffDays + " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
    }/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
    siteTime();
</script>

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <script type="text/javascript"> var OriginTitile = document.title, st; document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ喔哟，崩溃啦！", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) })
    </script>

    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', '');
</script>



    
    <script src="/libs/others/clicklove.js"></script>
    

    

    <!-- 雪花特效 -->
    

</body>

</html>